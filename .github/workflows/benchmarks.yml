name: benchmark_compare

on:
  # Triggers the workflow on push or pull request events but only for the "main" branch
  push:
    branches: [ "main" ]

jobs:
  # this job will list the benchmark files and store them as a json array for later use
  list-benchmarks:
    # The type of runner that the job will run on
    runs-on: ubuntu-latest
    # define an output from this job that can be read by other jobs
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      # check out just the benchmarks directory
      - uses: snow-actions/sparse-checkout@v1.1.0
        with:
          patterns: benchmarks
      # list the benchmark files and store them to the matrix output
      - id: set-matrix
        run: cd benchmarks && echo "::set-output name=matrix::$(ls | jq -R -s -c 'split("\n")[:-1]')"

  # this job will run each benchmark and compare to the previous values
  bench:
    # run after list-benchmarks finishes
    needs: list-benchmarks
    runs-on: ubuntu-latest
    # parallelize this job per benchmark script
    strategy:
        matrix:
            benchmark: ${{ fromJson(needs.list-benchmarks.outputs.matrix) }}
        # don't cancel other benchmarks when one fails
        fail-fast: false
  
    # Steps represent a sequence of tasks that will be executed as part of the job
    steps:
      - uses: actions/checkout@v3
        with:
          # TODO remove this, is only required for fake commit_count benchmark
          fetch-depth: 0
      - name: "Run benchmark: ${{ matrix.benchmark }}"
        run: |
          mkdir -p "${{ runner.temp }}/current" && "benchmarks/${{ matrix.benchmark }}" > "${{ runner.temp	}}/current/${{ matrix.benchmark }}"

      - name: Fetch this benchmark from the last successful workflow
        # TODO smarter error handling
        continue-on-error: true
        uses: dawidd6/action-download-artifact@v2
        with:
          # Optional, the status or conclusion of a completed workflow to search for
          # Can be one of a workflow conclusion:
          #   "failure", "success", "neutral", "cancelled", "skipped", "timed_out", "action_required"
          # Or a workflow status:
          #   "completed", "in_progress", "queued"
          ### success is too narrow if we fail the job when there are regressions
          ### completed is too broad, it will select a run with a startup failure and no artifacts to fetch
          ### FIXME ^^
          workflow_conclusion: completed
          # Optional, will use the branch
          branch: main
          # Optional, defaults to all types
          event: push
          # Uploaded artifact name
          name: ${{ matrix.benchmark }}
                # Optional, a directory where to extract artifact(s), defaults to the current directory
          path: ${{ runner.temp	}}/previous
        
      - name: Upload the new benchmark results to an artifact
        uses: actions/upload-artifact@v3
        with:
          name: ${{ matrix.benchmark }}
          path: ${{ runner.temp	}}/current/${{ matrix.benchmark }}

      # this comes last so that the upload will still happen if this fails
      - name: Compare current and previous bencmarks
        # TODO smarter error handling
        continue-on-error: true
        # only run this step if all previous steps completed without error
        if: ${{ success() }}
        # use jq to produce a comparison between the prev and curr benchmarks
        run: >
          jq
          --argfile prev ${{ runner.temp }}/previous/${{ matrix.benchmark }}
          --argfile curr ${{ runner.temp }}/current/${{ matrix.benchmark }}
          -n
          '
            $prev
            | [
                keys[]
                | if ($curr[.]|type)=="number" and ($prev[.]|type)=="number" then
                    {"\(.)": (($curr[.]/$prev[.]-1)*100) }
                  else
                    {"\(.)": ($prev[.] + " -> " + $curr[.])}
                  end
              ]
            | add
          ' | tee "${{ runner.temp}}/delta"
      
      - name: Output comparison results
        if: ${{ success() }}
        id: results
        run: |
          DELTA="$(cat ${{ runner.temp }}/delta)"
          DELTA="${DELTA//'%'/%25}"
          DELTA="${DELTA//$'\n'/%0A}"   
          DELTA="${DELTA//$'\r'/%0D}"
          echo "::set-output name=delta::$DELTA"

      - name: Regression Mean
        if: always() && fromJSON(steps.results.outputs.delta).mean > 2
        uses: actions/github-script@v3 # provides better custom error output than `echo "::error::" && exit 1`
        with:
          script: |
            core.error('Mean increased by ${{ fromJSON(steps.results.outputs.delta).mean }}%')

      - name: Regression Median
        if: always() && fromJSON(steps.results.outputs.delta).pct50 > 2
        uses: actions/github-script@v3
        with:
          script: |
            core.error('Median increased by ${{ fromJSON(steps.results.outputs.delta).pct50 }}%')

      - name: Regression 90th Percentile
        if: always() && fromJSON(steps.results.outputs.delta).pct90 > 2
        uses: actions/github-script@v3
        with:
          script: |
            core.error('90th Percentile increased by ${{ fromJSON(steps.results.outputs.delta).pct90 }}%')
